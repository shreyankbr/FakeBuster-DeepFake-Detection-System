{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:05.436385Z",
     "iopub.status.busy": "2025-10-21T07:18:05.436036Z",
     "iopub.status.idle": "2025-10-21T07:18:21.585057Z",
     "shell.execute_reply": "2025-10-21T07:18:21.584238Z",
     "shell.execute_reply.started": "2025-10-21T07:18:05.436351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id class subtype                                          video_dir  \\\n",
      "0      000  real    real  /kaggle/input/faceforencispp-extracted-frames/...   \n",
      "1      001  real    real  /kaggle/input/faceforencispp-extracted-frames/...   \n",
      "2      002  real    real  /kaggle/input/faceforencispp-extracted-frames/...   \n",
      "3      003  real    real  /kaggle/input/faceforencispp-extracted-frames/...   \n",
      "4      004  real    real  /kaggle/input/faceforencispp-extracted-frames/...   \n",
      "\n",
      "   n_frames  \n",
      "0        32  \n",
      "1        32  \n",
      "2        32  \n",
      "3        32  \n",
      "4        32   \n",
      "\n",
      "Total videos: 5995\n",
      "class\n",
      "fake    4996\n",
      "real     999\n",
      "Name: count, dtype: int64\n",
      "subtype\n",
      "FaceSwap          1000\n",
      "real               999\n",
      "Deepfakes          999\n",
      "Face2Face          999\n",
      "FaceShifter        999\n",
      "NeuralTextures     999\n",
      "Name: count, dtype: int64\n",
      "Saved manifest ‚Üí /kaggle/working/ffpp_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "BASE = Path(\"/kaggle/input/faceforencispp-extracted-frames\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "# real videos\n",
    "for vid_dir in sorted((BASE/\"real\").iterdir()):\n",
    "    if not vid_dir.is_dir(): continue\n",
    "    frames = sorted(list(vid_dir.glob(\"*.jpg\")) + list(vid_dir.glob(\"*.png\")))\n",
    "    if not frames: continue\n",
    "    rows.append({\n",
    "        \"video_id\": vid_dir.name,\n",
    "        \"class\": \"real\",\n",
    "        \"subtype\": \"real\",\n",
    "        \"video_dir\": str(vid_dir),\n",
    "        \"n_frames\": len(frames)\n",
    "    })\n",
    "\n",
    "# fake videos (5 subtypes)\n",
    "for subtype_dir in sorted((BASE/\"fake\").iterdir()):\n",
    "    if not subtype_dir.is_dir(): continue\n",
    "    subtype = subtype_dir.name\n",
    "    for vid_dir in sorted(subtype_dir.iterdir()):\n",
    "        if not vid_dir.is_dir(): continue\n",
    "        frames = sorted(list(vid_dir.glob(\"*.jpg\")) + list(vid_dir.glob(\"*.png\")))\n",
    "        if not frames: continue\n",
    "        rows.append({\n",
    "            \"video_id\": f\"{subtype}/{vid_dir.name}\",\n",
    "            \"class\": \"fake\",\n",
    "            \"subtype\": subtype,\n",
    "            \"video_dir\": str(vid_dir),\n",
    "            \"n_frames\": len(frames)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Total videos:\", len(df))\n",
    "print(df[\"class\"].value_counts())\n",
    "print(df[\"subtype\"].value_counts())\n",
    "\n",
    "df.to_csv(\"/kaggle/working/ffpp_manifest.csv\", index=False)\n",
    "print(\"Saved manifest ‚Üí /kaggle/working/ffpp_manifest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:21.587001Z",
     "iopub.status.busy": "2025-10-21T07:18:21.586452Z",
     "iopub.status.idle": "2025-10-21T07:18:21.591861Z",
     "shell.execute_reply": "2025-10-21T07:18:21.591127Z",
     "shell.execute_reply.started": "2025-10-21T07:18:21.586975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FFT function with normalization\n",
    "import torch\n",
    "\n",
    "def to_fft_tensor(img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert RGB tensor to FFT magnitude spectrum\n",
    "    img_tensor: [3, H, W] normalized tensor\n",
    "    returns: [1, H, W] normalized FFT magnitude\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = img_tensor.mean(dim=0, keepdim=True) \n",
    "    \n",
    "    # 2D FFT\n",
    "    fft = torch.fft.fft2(gray)\n",
    "    fft_shift = torch.fft.fftshift(fft)\n",
    "    magnitude = torch.abs(fft_shift)\n",
    "    \n",
    "    # Log scale for better dynamic range\n",
    "    log_mag = torch.log1p(magnitude)\n",
    "    \n",
    "    # Robust normalization\n",
    "    min_val = log_mag.min()\n",
    "    max_val = log_mag.max()\n",
    "    \n",
    "    if (max_val - min_val) > 1e-8:\n",
    "        log_mag = (log_mag - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        log_mag = torch.zeros_like(log_mag)\n",
    "    \n",
    "    return log_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:21.592804Z",
     "iopub.status.busy": "2025-10-21T07:18:21.592585Z",
     "iopub.status.idle": "2025-10-21T07:18:25.726089Z",
     "shell.execute_reply": "2025-10-21T07:18:25.725386Z",
     "shell.execute_reply.started": "2025-10-21T07:18:21.592788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4795 videos | Val: 1200 videos\n",
      "Train class balance: {'fake': 3996, 'real': 799}\n",
      "Val class balance: {'fake': 1000, 'real': 200}\n",
      "Class counts: {'fake': 3996, 'real': 799}\n",
      "‚úÖ DataLoaders ready | train videos: 4795 | val videos: 1200\n",
      "Class balance in train set: {'fake': 3996, 'real': 799}\n"
     ]
    }
   ],
   "source": [
    "# Dataset with frame sampling and class balancing\n",
    "import os, random, math, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MANIFEST = Path(\"/kaggle/working/ffpp_manifest.csv\")\n",
    "IMG_SIZE = 224\n",
    "FRAMES_PER_VIDEO = 12\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def list_frames(video_dir: Path) -> List[Path]:\n",
    "    frames = sorted(video_dir.glob(\"*.jpg\")) + sorted(video_dir.glob(\"*.png\"))\n",
    "    return frames\n",
    "\n",
    "# Frame sampling for temporal coverage\n",
    "def sample_frames(n_frames: int, num_samples: int, train: bool) -> List[int]:\n",
    "    \"\"\"Sample frames with better temporal coverage\"\"\"\n",
    "    if n_frames <= 0:\n",
    "        return []\n",
    "    \n",
    "    if n_frames <= num_samples:\n",
    "        indices = list(range(n_frames))\n",
    "        # Pad with last frame if needed\n",
    "        while len(indices) < num_samples:\n",
    "            indices.append(indices[-1])\n",
    "        return indices\n",
    "    \n",
    "    if train:\n",
    "        # During training: random sampling but with temporal spread\n",
    "        segments = np.linspace(0, n_frames - 1, num_samples + 1, dtype=int)\n",
    "        indices = []\n",
    "        for i in range(num_samples):\n",
    "            start, end = segments[i], segments[i + 1]\n",
    "            indices.append(random.randint(start, end - 1) if start < end else start)\n",
    "        return sorted(indices)\n",
    "    else:\n",
    "        # During validation: uniform sampling\n",
    "        return np.linspace(0, n_frames - 1, num_samples, dtype=int).tolist()\n",
    "\n",
    "# ---------- Enhanced Dataset ----------\n",
    "class FrameVideoDataset(Dataset):\n",
    "    def __init__(self, df, train=True, frames_per_video=32, img_size=224):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.train = train\n",
    "        self.frames_per_video = frames_per_video\n",
    "\n",
    "        aug = []\n",
    "        if train:\n",
    "            aug += [\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "                T.RandomGrayscale(p=0.1),\n",
    "                T.RandomApply([T.GaussianBlur(3)], p=0.1),\n",
    "            ]\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            *aug,\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        vid_dir = Path(row[\"video_dir\"])\n",
    "        frames = sorted(list(vid_dir.glob(\"*.jpg\")) + list(vid_dir.glob(\"*.png\")))\n",
    "\n",
    "        # Sampling\n",
    "        chosen = sample_frames(len(frames), self.frames_per_video, self.train)\n",
    "        rgb_imgs, fft_imgs = [], []\n",
    "\n",
    "        for i in chosen:\n",
    "            with Image.open(frames[i]) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "                rgb = self.tf(im)  \n",
    "                rgb_imgs.append(rgb)\n",
    "                fft_map = to_fft_tensor(rgb)  \n",
    "                fft_imgs.append(fft_map)\n",
    "\n",
    "        rgb_imgs = torch.stack(rgb_imgs, dim=0) \n",
    "        fft_imgs = torch.stack(fft_imgs, dim=0) \n",
    "\n",
    "        label = torch.tensor(1 if row[\"class\"] == \"fake\" else 0, dtype=torch.long)\n",
    "        meta = {\n",
    "            \"video_id\": row[\"video_id\"],\n",
    "            \"class\": row[\"class\"],\n",
    "            \"subtype\": row[\"subtype\"],\n",
    "        }\n",
    "        return rgb_imgs, fft_imgs, label, meta\n",
    "\n",
    "# Split with balanced subtype representation\n",
    "def make_balanced_splits(df: pd.DataFrame, val_ratio=0.2, seed=42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create splits with balanced representation of all manipulation types\"\"\"\n",
    "    train_dfs, val_dfs = [], []\n",
    "    \n",
    "    # Split each subtype separately to ensure balance\n",
    "    for subtype in df['subtype'].unique():\n",
    "        subtype_df = df[df['subtype'] == subtype].copy()\n",
    "        if len(subtype_df) > 1:\n",
    "            subtype_train, subtype_val = train_test_split(\n",
    "                subtype_df, test_size=val_ratio, random_state=seed, stratify=subtype_df['class']\n",
    "            )\n",
    "        else:\n",
    "            subtype_train, subtype_val = subtype_df, subtype_df.iloc[:0]\n",
    "        \n",
    "        train_dfs.append(subtype_train)\n",
    "        val_dfs.append(subtype_val)\n",
    "    \n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"Train: {len(train_df)} videos | Val: {len(val_df)} videos\")\n",
    "    print(f\"Train class balance: {train_df['class'].value_counts().to_dict()}\")\n",
    "    print(f\"Val class balance: {val_df['class'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "# ---------- Build splits + loaders ----------\n",
    "full = pd.read_csv(MANIFEST)\n",
    "train_df, val_df = make_balanced_splits(full, val_ratio=0.2, seed=42)\n",
    "\n",
    "train_ds = FrameVideoDataset(train_df, train=True,  frames_per_video=FRAMES_PER_VIDEO)\n",
    "val_ds   = FrameVideoDataset(val_df,   train=False, frames_per_video=FRAMES_PER_VIDEO)  \n",
    "\n",
    "class_counts = train_df[\"class\"].value_counts().to_dict()\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "\n",
    "# Give more weight to real class to handle imbalance\n",
    "weights = []\n",
    "for _, row in train_df.iterrows():\n",
    "    if row[\"class\"] == \"real\":\n",
    "        weights.append(3.0 / class_counts[\"real\"])  # Higher weight for real class\n",
    "    else:\n",
    "        weights.append(1.0 / class_counts[\"fake\"])\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready | train videos: {len(train_ds)} | val videos: {len(val_ds)}\")\n",
    "print(f\"Class balance in train set: {train_df['class'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:25.727757Z",
     "iopub.status.busy": "2025-10-21T07:18:25.727527Z",
     "iopub.status.idle": "2025-10-21T07:18:27.895924Z",
     "shell.execute_reply": "2025-10-21T07:18:27.895197Z",
     "shell.execute_reply.started": "2025-10-21T07:18:25.727740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb shape: torch.Size([4, 12, 3, 224, 224])\n",
      "fft shape: torch.Size([4, 12, 1, 224, 224])\n",
      "labels shape: torch.Size([4])\n",
      "sample labels: [0, 0, 0, 0]\n",
      "meta[0]: {'video_id': '195', 'class': 'real', 'subtype': 'real'}\n",
      "Successfully moved to cuda\n",
      "Batch size: 4, Frames: 12\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch = next(iter(train_loader))\n",
    "rgb, fft, labels, metas = batch\n",
    "\n",
    "print(\"rgb shape:\", rgb.shape)   \n",
    "print(\"fft shape:\", fft.shape)  \n",
    "print(\"labels shape:\", labels.shape) \n",
    "print(\"sample labels:\", labels.tolist())\n",
    "print(\"meta[0]:\", {k: v[0] for k, v in metas.items()})\n",
    "\n",
    "# Test GPU memory\n",
    "rgb = rgb.to(device, non_blocking=True)\n",
    "fft = fft.to(device, non_blocking=True)\n",
    "labels = labels.to(device, non_blocking=True)\n",
    "print(\"Successfully moved to\", device)\n",
    "print(f\"Batch size: {rgb.shape[0]}, Frames: {rgb.shape[1]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:27.897757Z",
     "iopub.status.busy": "2025-10-21T07:18:27.897373Z",
     "iopub.status.idle": "2025-10-21T07:18:31.961372Z",
     "shell.execute_reply": "2025-10-21T07:18:31.960780Z",
     "shell.execute_reply.started": "2025-10-21T07:18:27.897732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dual-branch model with batch norm handling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class DualBranchEfficientNet(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_classes=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # RGB branch ‚Üí EfficientNet-B4\n",
    "        self.rgb_backbone = timm.create_model(\n",
    "            \"tf_efficientnet_b4_ns\", pretrained=True, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        rgb_dim = self.rgb_backbone.num_features\n",
    "        self.rgb_proj = nn.Sequential(\n",
    "            nn.Linear(rgb_dim, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # FFT branch ‚Üí EfficientNet-B0\n",
    "        self.fft_backbone = timm.create_model(\n",
    "            \"tf_efficientnet_b0_ns\", pretrained=True, in_chans=1, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        fft_dim = self.fft_backbone.num_features\n",
    "        self.fft_proj = nn.Sequential(\n",
    "            nn.Linear(fft_dim, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, fft):\n",
    "        B, F, _, H, W = rgb.shape\n",
    "\n",
    "        # RGB branch\n",
    "        rgb = rgb.view(B * F, 3, H, W)\n",
    "        rgb_feats = self.rgb_backbone(rgb)\n",
    "        rgb_feats = self.rgb_proj(rgb_feats)\n",
    "        rgb_feats = rgb_feats.view(B, F, -1).permute(0, 2, 1)\n",
    "        rgb_pooled = self.temporal_pool(rgb_feats).squeeze(-1)\n",
    "\n",
    "        # FFT branch\n",
    "        fft = fft.view(B * F, 1, H, W)\n",
    "        fft_feats = self.fft_backbone(fft)\n",
    "        fft_feats = self.fft_proj(fft_feats)\n",
    "        fft_feats = fft_feats.view(B, F, -1).permute(0, 2, 1)\n",
    "        fft_pooled = self.temporal_pool(fft_feats).squeeze(-1)\n",
    "\n",
    "        # Fused features\n",
    "        fused = torch.cat([rgb_pooled, fft_pooled], dim=1)\n",
    "        out = self.classifier(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:31.962437Z",
     "iopub.status.busy": "2025-10-21T07:18:31.962164Z",
     "iopub.status.idle": "2025-10-21T07:18:34.593234Z",
     "shell.execute_reply": "2025-10-21T07:18:34.592426Z",
     "shell.execute_reply.started": "2025-10-21T07:18:31.962413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b402b7ac17ac4c319c5869667406cdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82caba71eb14e70b199df5bd64034d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts - Real: 799, Fake: 3996\n",
      "pos_weight: 5.00 (should be >1)\n",
      "‚úÖ Training setup complete!\n",
      "Device: cuda\n",
      "Model parameters: 23,656,837\n",
      "pos_weight: 5.00x (should be ~5.0x)\n",
      "Training samples: 4795\n",
      "Validation samples: 1200\n",
      "Frames per video: 12 | Batch size: 4\n",
      "üíæ Will save ALL models (not just best)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model with enhanced architecture\n",
    "model = DualBranchEfficientNet(embed_dim=512, num_classes=1, dropout=0.3).to(device)\n",
    "\n",
    "# Class weight calculation\n",
    "real_count = train_df['class'].value_counts()['real']\n",
    "fake_count = train_df['class'].value_counts()['fake']\n",
    "pos_weight = torch.tensor([fake_count / real_count]).to(device)  \n",
    "print(f\"Class counts - Real: {real_count}, Fake: {fake_count}\")\n",
    "print(f\"pos_weight: {pos_weight.item():.2f} (should be >1)\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "training_history = []\n",
    "\n",
    "def train_one_epoch(loader, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    all_labels, all_probs, all_preds = [], [], []\n",
    "\n",
    "    for batch_idx, (rgb, fft, labels, _) in enumerate(loader):\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)}\")\n",
    "            \n",
    "        rgb, fft = rgb.to(device), fft.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(rgb, fft)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item() * rgb.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Handle AUC calculation\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    else:\n",
    "        auc = 0.0\n",
    "    \n",
    "    # Handle F1 calculation\n",
    "    if len(np.unique(all_preds)) > 1:\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "    \n",
    "    return epoch_loss / len(loader.dataset), acc, auc, f1, precision, recall\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(loader, epoch):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    all_labels, all_probs, all_preds = [], [], []\n",
    "\n",
    "    for rgb, fft, labels, _ in loader:\n",
    "        rgb, fft = rgb.to(device), fft.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(rgb, fft)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        epoch_loss += loss.item() * rgb.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy().flatten()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Handle AUC calculation\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    else:\n",
    "        auc = 0.0\n",
    "    \n",
    "    # Handle F1 calculation\n",
    "    if len(np.unique(all_preds)) > 1:\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "\n",
    "    return epoch_loss / len(loader.dataset), acc, auc, f1, precision, recall\n",
    "\n",
    "print(\"‚úÖ Training setup complete!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"pos_weight: {pos_weight.item():.2f}x (should be ~5.0x)\")\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Frames per video: {FRAMES_PER_VIDEO} | Batch size: {batch_size}\")\n",
    "print(\"üíæ Will save ALL models (not just best)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:18:34.594382Z",
     "iopub.status.busy": "2025-10-21T07:18:34.594153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced training with class balancing...\n",
      "Real class weight: 5.00x\n",
      "üíæ Will save ALL models after each epoch\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 1]\n",
      "    Train ‚Üí Loss: 1.1248 | Acc: 0.5620 | AUC: 0.7479 | F1: 0.4899\n",
      "    Val   ‚Üí Loss: 2.5501 | Acc: 0.7233 | AUC: 0.9070 | F1: 0.8042\n",
      "    Val Details ‚Üí Precision: 0.9799 | Recall: 0.6820\n",
      "üíæ Saved model for epoch 1 ‚Üí AUC: 0.9070 | Path: /kaggle/working/model_epoch_1.pth\n",
      "üèÜ New best AUC: 0.9070\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 2]\n",
      "    Train ‚Üí Loss: 0.7576 | Acc: 0.7938 | AUC: 0.9053 | F1: 0.6860\n",
      "    Val   ‚Üí Loss: 1.1329 | Acc: 0.8908 | AUC: 0.9156 | F1: 0.9334\n",
      "    Val Details ‚Üí Precision: 0.9493 | Recall: 0.9180\n",
      "üíæ Saved model for epoch 2 ‚Üí AUC: 0.9156 | Path: /kaggle/working/model_epoch_2.pth\n",
      "üèÜ New best AUC: 0.9156\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 3]\n",
      "    Train ‚Üí Loss: 0.6189 | Acc: 0.8497 | AUC: 0.9372 | F1: 0.7503\n",
      "    Val   ‚Üí Loss: 0.5068 | Acc: 0.9150 | AUC: 0.9378 | F1: 0.9496\n",
      "    Val Details ‚Üí Precision: 0.9385 | Recall: 0.9610\n",
      "üíæ Saved model for epoch 3 ‚Üí AUC: 0.9378 | Path: /kaggle/working/model_epoch_3.pth\n",
      "üèÜ New best AUC: 0.9378\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 4]\n",
      "    Train ‚Üí Loss: 0.5330 | Acc: 0.8792 | AUC: 0.9533 | F1: 0.7883\n",
      "    Val   ‚Üí Loss: 0.3723 | Acc: 0.9158 | AUC: 0.9591 | F1: 0.9511\n",
      "    Val Details ‚Üí Precision: 0.9213 | Recall: 0.9830\n",
      "üíæ Saved model for epoch 4 ‚Üí AUC: 0.9591 | Path: /kaggle/working/model_epoch_4.pth\n",
      "üèÜ New best AUC: 0.9591\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 5]\n",
      "    Train ‚Üí Loss: 0.4310 | Acc: 0.9073 | AUC: 0.9685 | F1: 0.8325\n",
      "    Val   ‚Üí Loss: 0.4681 | Acc: 0.9458 | AUC: 0.9678 | F1: 0.9672\n",
      "    Val Details ‚Üí Precision: 0.9756 | Recall: 0.9590\n",
      "üíæ Saved model for epoch 5 ‚Üí AUC: 0.9678 | Path: /kaggle/working/model_epoch_5.pth\n",
      "üèÜ New best AUC: 0.9678\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 6]\n",
      "    Train ‚Üí Loss: 0.3970 | Acc: 0.9261 | AUC: 0.9732 | F1: 0.8635\n",
      "    Val   ‚Üí Loss: 0.3343 | Acc: 0.9408 | AUC: 0.9717 | F1: 0.9650\n",
      "    Val Details ‚Üí Precision: 0.9505 | Recall: 0.9800\n",
      "üíæ Saved model for epoch 6 ‚Üí AUC: 0.9717 | Path: /kaggle/working/model_epoch_6.pth\n",
      "üèÜ New best AUC: 0.9717\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 7]\n",
      "    Train ‚Üí Loss: 0.3570 | Acc: 0.9322 | AUC: 0.9777 | F1: 0.8737\n",
      "    Val   ‚Üí Loss: 0.3534 | Acc: 0.9417 | AUC: 0.9659 | F1: 0.9654\n",
      "    Val Details ‚Üí Precision: 0.9550 | Recall: 0.9760\n",
      "üíæ Saved model for epoch 7 ‚Üí AUC: 0.9659 | Path: /kaggle/working/model_epoch_7.pth\n",
      "‚ö†Ô∏è No AUC improvement for 1/4 epochs.\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 8]\n",
      "    Train ‚Üí Loss: 0.3349 | Acc: 0.9386 | AUC: 0.9799 | F1: 0.8796\n",
      "    Val   ‚Üí Loss: 0.7553 | Acc: 0.9225 | AUC: 0.9782 | F1: 0.9521\n",
      "    Val Details ‚Üí Precision: 0.9809 | Recall: 0.9250\n",
      "üíæ Saved model for epoch 8 ‚Üí AUC: 0.9782 | Path: /kaggle/working/model_epoch_8.pth\n",
      "üèÜ New best AUC: 0.9782\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 9]\n",
      "    Train ‚Üí Loss: 0.2890 | Acc: 0.9478 | AUC: 0.9849 | F1: 0.8986\n",
      "    Val   ‚Üí Loss: 0.5206 | Acc: 0.9442 | AUC: 0.9790 | F1: 0.9660\n",
      "    Val Details ‚Üí Precision: 0.9794 | Recall: 0.9530\n",
      "üíæ Saved model for epoch 9 ‚Üí AUC: 0.9790 | Path: /kaggle/working/model_epoch_9.pth\n",
      "üèÜ New best AUC: 0.9790\n",
      "Batch 0/1198\n",
      "Batch 200/1198\n",
      "Batch 400/1198\n",
      "Batch 600/1198\n",
      "Batch 800/1198\n",
      "Batch 1000/1198\n",
      "\n",
      "[Epoch 10]\n",
      "    Train ‚Üí Loss: 0.2642 | Acc: 0.9524 | AUC: 0.9869 | F1: 0.9093\n",
      "    Val   ‚Üí Loss: 0.8777 | Acc: 0.9142 | AUC: 0.9396 | F1: 0.9478\n",
      "    Val Details ‚Üí Precision: 0.9609 | Recall: 0.9350\n",
      "üíæ Saved model for epoch 10 ‚Üí AUC: 0.9396 | Path: /kaggle/working/model_epoch_10.pth\n",
      "‚ö†Ô∏è No AUC improvement for 1/4 epochs.\n",
      "Batch 0/1198\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 15\n",
    "PATIENCE = 4\n",
    "epochs_no_improve = 0\n",
    "base_save_path = \"/kaggle/working/model_epoch_{}.pth\"\n",
    "\n",
    "# Ensure required variables are initialized\n",
    "training_history = []\n",
    "assert 'pos_weight' in globals(), \"pos_weight must be defined\"\n",
    "assert 'train_loader' in globals(), \"train_loader must be defined\"\n",
    "assert 'val_loader' in globals(), \"val_loader must be defined\"\n",
    "assert 'model' in globals(), \"model must be defined\"\n",
    "assert 'optimizer' in globals(), \"optimizer must be defined\"\n",
    "assert 'scaler' in globals(), \"scaler must be defined\"\n",
    "\n",
    "print(\"Starting enhanced training with class balancing...\")\n",
    "print(f\"Real class weight: {pos_weight.item():.2f}x\")\n",
    "print(\"üíæ Will save ALL models after each epoch\")\n",
    "\n",
    "# Track best metrics for early stopping (but save all models)\n",
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc, train_auc, train_f1, train_prec, train_rec = train_one_epoch(train_loader, epoch)\n",
    "    # Validate\n",
    "    val_loss, val_acc, val_auc, val_f1, val_prec, val_rec = validate(val_loader, epoch)\n",
    "\n",
    "    # Store history\n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss, 'train_acc': train_acc, 'train_auc': train_auc, 'train_f1': train_f1,\n",
    "        'val_loss': val_loss, 'val_acc': val_acc, 'val_auc': val_auc, 'val_f1': val_f1,\n",
    "        'val_precision': val_prec, 'val_recall': val_rec\n",
    "    })\n",
    "\n",
    "    print(f\"\"\"\\n[Epoch {epoch}]\n",
    "    Train ‚Üí Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | AUC: {train_auc:.4f} | F1: {train_f1:.4f}\n",
    "    Val   ‚Üí Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | AUC: {val_auc:.4f} | F1: {val_f1:.4f}\n",
    "    Val Details ‚Üí Precision: {val_prec:.4f} | Recall: {val_rec:.4f}\"\"\")\n",
    "\n",
    "    # Save model after every epoch\n",
    "    save_path = base_save_path.format(epoch)\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"val_precision\": val_prec,\n",
    "        \"val_recall\": val_rec,\n",
    "        \"training_history\": training_history\n",
    "    }, save_path)\n",
    "    print(f\"üíæ Saved model for epoch {epoch} ‚Üí AUC: {val_auc:.4f} | Path: {save_path}\")\n",
    "\n",
    "    # Track best AUC only for early stopping (not for saving)\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"üèÜ New best AUC: {val_auc:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"‚ö†Ô∏è No AUC improvement for {epochs_no_improve}/{PATIENCE} epochs.\")\n",
    "\n",
    "    # Early stopping (optional - keeps training but stops if no improvement)\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"‚èπ Early stopping at epoch {epoch}. Best val_auc = {best_auc:.4f}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete. Best val_auc = {best_auc:.4f}\")\n",
    "print(f\"üìÅ All models saved to /kaggle/working/model_epoch_*.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7790941,
     "sourceId": 12366330,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
